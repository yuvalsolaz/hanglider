{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvalsolaz/hanglider/blob/main/text2image_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9f9e9041",
      "metadata": {
        "id": "9f9e9041"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from PIL import Image\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from IPython import display\n",
        "from google.colab import files "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipyplot\n",
        "import ipyplot"
      ],
      "metadata": {
        "id": "eoADXzsQ8e1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec98ff6c-80a0-49ed-cad4-8da30399c2ac"
      },
      "id": "eoADXzsQ8e1O",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipyplot in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from ipyplot) (7.1.2)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.7/dist-packages (from ipyplot) (1.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ipyplot) (1.21.5)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from ipyplot) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->ipyplot) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->ipyplot) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->ipyplot) (0.7.0)\n",
            "\n",
            "        WARNING! Google Colab Environment detected!\n",
            "        You might encounter issues while running in Google Colab environment.\n",
            "        If images are not displaying properly please try setting `force_b64` param to `True`.\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88696845",
      "metadata": {
        "id": "88696845",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db99c9c-d3a8-4cd4-edb5-09ea9cd613d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using: cuda device\n",
            "\n"
          ]
        }
      ],
      "source": [
        "has_cuda = th.cuda.is_available()\n",
        "device = th.device('cpu' if not has_cuda else 'cuda')\n",
        "print(f'using: {device} device\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b3ae48a1",
      "metadata": {
        "id": "b3ae48a1"
      },
      "outputs": [],
      "source": [
        "# Sampling parameters\n",
        "batch_size = 1\n",
        "guidance_scale = 3.0\n",
        "diffusion_steps = {'base': '100',       # use diffusion steps for fast sampling   in base model \n",
        "                   'upsample':'fast27'} # use upsample diffusion steps for very fast sampling in upsampling model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/glide-text2im"
      ],
      "metadata": {
        "id": "Pru8qLh4bRD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e101f3-9f90-4180-a47f-b1ce3826254d"
      },
      "id": "Pru8qLh4bRD7",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/glide-text2im\n",
            "  Cloning https://github.com/openai/glide-text2im to /tmp/pip-req-build-_36f4ahj\n",
            "  Running command git clone -q https://github.com/openai/glide-text2im /tmp/pip-req-build-_36f4ahj\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (21.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (4.63.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glide-text2im==0.0.0) (1.21.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->glide-text2im==0.0.0) (0.2.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->glide-text2im==0.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->glide-text2im==0.0.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->glide-text2im==0.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->glide-text2im==0.0.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->glide-text2im==0.0.0) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf636cc",
      "metadata": {
        "id": "bbf636cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03aac21-d685-4ad1-fc15-ead9909d431e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use {'base': '100', 'upsample': 'fast27'} diffusion steps for fast sampling\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text2ImUNet(\n",
              "  (time_embed): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=768, bias=True)\n",
              "    (1): SiLU()\n",
              "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
              "  )\n",
              "  (input_blocks): ModuleList(\n",
              "    (0): TimestepEmbedSequential(\n",
              "      (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (2): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (3): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (4): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Downsample(\n",
              "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (x_upd): Downsample(\n",
              "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (5): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (6): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (7): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (8): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Downsample(\n",
              "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (x_upd): Downsample(\n",
              "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (9): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (10): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (11): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (12): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Downsample(\n",
              "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (x_upd): Downsample(\n",
              "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (13): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(576, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (14): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (15): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (middle_block): TimestepEmbedSequential(\n",
              "    (0): ResBlock(\n",
              "      (in_layers): Sequential(\n",
              "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (1): Identity()\n",
              "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (h_upd): Identity()\n",
              "      (x_upd): Identity()\n",
              "      (emb_layers): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "      )\n",
              "      (out_layers): Sequential(\n",
              "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (1): SiLU()\n",
              "        (2): Dropout(p=0.1, inplace=False)\n",
              "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (skip_connection): Identity()\n",
              "    )\n",
              "    (1): AttentionBlock(\n",
              "      (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "      (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "      (attention): QKVAttention()\n",
              "      (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "      (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (2): ResBlock(\n",
              "      (in_layers): Sequential(\n",
              "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (1): Identity()\n",
              "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (h_upd): Identity()\n",
              "      (x_upd): Identity()\n",
              "      (emb_layers): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "      )\n",
              "      (out_layers): Sequential(\n",
              "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (1): SiLU()\n",
              "        (2): Dropout(p=0.1, inplace=False)\n",
              "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (skip_connection): Identity()\n",
              "    )\n",
              "  )\n",
              "  (output_blocks): ModuleList(\n",
              "    (0): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (1): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (2): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1536, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (3): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1344, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1344, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1344, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (2): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Upsample()\n",
              "        (x_upd): Upsample()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1536, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (4): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1344, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1344, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1344, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (5): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1152, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1152, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (6): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1152, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(1152, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (7): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(960, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (2): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Upsample()\n",
              "        (x_upd): Upsample()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=1152, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (8): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(960, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (9): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (10): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "    )\n",
              "    (11): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): AttentionBlock(\n",
              "        (norm): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n",
              "        (attention): QKVAttention()\n",
              "        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n",
              "        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (2): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Upsample()\n",
              "        (x_upd): Upsample()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (12): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 576, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (13): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (14): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (15): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
              "          (1): Identity()\n",
              "          (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (h_upd): Identity()\n",
              "        (x_upd): Identity()\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=768, out_features=384, bias=True)\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): GroupNorm32(32, 192, eps=1e-05, affine=True)\n",
              "    (1): Identity()\n",
              "    (2): Conv2d(192, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): ModuleList(\n",
              "      (0): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (12): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (13): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (14): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (15): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention): QKVMultiheadAttention()\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (gelu): GELU()\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  (token_embedding): Embedding(50257, 512)\n",
              "  (transformer_proj): Linear(in_features=512, out_features=768, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Load / Create base model.\n",
        "from glide_text2im.download import load_checkpoint\n",
        "from glide_text2im.model_creation import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "\n",
        "print(f'use {diffusion_steps} diffusion steps for fast sampling')\n",
        "options = model_and_diffusion_defaults()\n",
        "options['use_fp16'] = has_cuda\n",
        "options['timestep_respacing'] = diffusion_steps['base']  \n",
        "model, diffusion = create_model_and_diffusion(**options)\n",
        "model.eval()\n",
        "if has_cuda:\n",
        "    model.convert_to_fp16()\n",
        "    model.to(device)\n",
        "    model.load_state_dict(load_checkpoint('base', device))\n",
        "print('total base parameters', sum(x.numel() for x in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aad6872",
      "metadata": {
        "id": "6aad6872"
      },
      "outputs": [],
      "source": [
        " # Load / Create upsampler model.\n",
        "from glide_text2im.model_creation import model_and_diffusion_defaults_upsampler\n",
        "\n",
        "options_up = model_and_diffusion_defaults_upsampler()\n",
        "options_up['use_fp16'] = has_cuda\n",
        "# options_up['timestep_respacing'] = diffusion_steps['upsample']\n",
        "\n",
        "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
        "model_up.eval()\n",
        "if has_cuda:\n",
        "    model_up.convert_to_fp16()\n",
        "    model_up.to(device)\n",
        "    model_up.load_state_dict(load_checkpoint('upsample', device))\n",
        "\n",
        "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pil_image(batch: th.Tensor):\n",
        "    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "    npimage = reshaped.numpy()\n",
        "    im = Image.fromarray(npimage)\n",
        "    return im.resize(size=(256,256))\n",
        "\n",
        "def create_pygame_image(batch_tuple: th.Tensor):\n",
        "    batch = batch_tuple[0]\n",
        "    scaled = ((batch + 1) * 127.5).round().clamp(0, 255).to(th.uint8).cpu()\n",
        "    reshaped = scaled.permute(1 ,2 ,0).reshape([batch.shape[2], -1, 3])\n",
        "    npimage = reshaped.numpy()\n",
        "    im = Image.fromarray(npimage)\n",
        "    return im.resize(size=(256,256))\n",
        "\n",
        "def image_show_pil(batch: th.Tensor, caption=''):\n",
        "    create_pil_image(batch).show(title=caption)"
      ],
      "metadata": {
        "id": "_tNRPx-zrYVB"
      },
      "id": "_tNRPx-zrYVB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d05a55b",
      "metadata": {
        "id": "8d05a55b"
      },
      "outputs": [],
      "source": [
        "image_array = []\n",
        "def generate_image(prompt:str):\n",
        "    # Tune this parameter to control the sharpness of 256x256 images.\n",
        "    # A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
        "    upsample_temp = 0.997\n",
        "\n",
        "    eps_arr = []\n",
        "    rest_arr = []\n",
        "    # Create a classifier-free guidance sampling function\n",
        "    def model_fn(x_t, ts, **kwargs):\n",
        "        half = x_t[: len(x_t) // 2]\n",
        "        combined = th.cat([half, half], dim=0)\n",
        "        model_out = model(combined, ts, **kwargs)\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        eps_arr.append(eps)\n",
        "        rest_arr.append(rest)\n",
        "        cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "        eps = th.cat([half_eps, half_eps], dim=0)\n",
        "        image = th.cat([eps, rest], dim=1)\n",
        "        image_array.append(create_pygame_image(rest))\n",
        "        \n",
        "        return image\n",
        "\n",
        "    ##############################\n",
        "    # Sample from the base model #\n",
        "    ##############################\n",
        "\n",
        "    # Create the text tokens to feed to the model.\n",
        "    tokens = model.tokenizer.encode(prompt)\n",
        "    tokens, mask = model.tokenizer.padded_tokens_and_mask(tokens, options['text_ctx'])\n",
        "\n",
        "    # Create the classifier-free guidance tokens (empty)\n",
        "    full_batch_size = batch_size * 2\n",
        "    uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask([], options['text_ctx'])\n",
        "\n",
        "    # Pack the tokens together into model kwargs.\n",
        "    model_kwargs = dict(\n",
        "        tokens=th.tensor(\n",
        "            [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
        "        ),\n",
        "        mask=th.tensor(\n",
        "            [mask] * batch_size + [uncond_mask] * batch_size,\n",
        "            dtype=th.bool,\n",
        "            device=device,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Sample from the base model.\n",
        "    model.del_cache()\n",
        "    samples = diffusion.p_sample_loop(\n",
        "        model_fn,\n",
        "        (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
        "        device=device,\n",
        "        clip_denoised=True,\n",
        "        progress=True,\n",
        "        model_kwargs=model_kwargs,\n",
        "        cond_fn=None,\n",
        "    )[:batch_size]\n",
        "    model.del_cache()\n",
        "    return samples, eps_arr, rest_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2958d20d",
      "metadata": {
        "id": "2958d20d"
      },
      "source": [
        "#### enter image caption for image generation: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gif(frames,  gif_name):\n",
        "    frame_one = frames[0]\n",
        "    steps = len(frames)\n",
        "    y = np.log(steps+1) - np.log(np.arange(1,steps)) \n",
        "    ijump = [int(y[:i].sum()) for i in range(steps)]     \n",
        "    gif_frames = [frames[i] for i in ijump]\n",
        "    gif_file = f\"{gif_name}.gif\"\n",
        "    frame_one.save(gif_file, format=\"GIF\", append_images=gif_frames,\n",
        "                   save_all=True, duration=100, loop=0)\n",
        "    return gif_file"
      ],
      "metadata": {
        "id": "FBR3BKXqYKKe"
      },
      "id": "FBR3BKXqYKKe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8c4c8b",
      "metadata": {
        "id": "ba8c4c8b"
      },
      "outputs": [],
      "source": [
        "prompt = input('enter image caption for image generation: ')\n",
        "print (f'generating image for: {prompt}')\n",
        "images = []\n",
        "for i in range(1):\n",
        "    samples,eps_arr, rest_arr  = generate_image(prompt=prompt)\n",
        "    im = create_pil_image(samples)\n",
        "    images.append(im)\n",
        "    ipyplot.plot_images([im])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gif_file_name = make_gif(gif_name=prompt,frames=image_array)"
      ],
      "metadata": {
        "id": "UPE4TmnLyRnm"
      },
      "id": "UPE4TmnLyRnm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display GIF in Jupyter, CoLab, IPython\n",
        "with open(gif_file_name,'rb') as f:\n",
        "    display.Image(data=f.read(), format='png')"
      ],
      "metadata": {
        "id": "PFXfwUvSOvGB"
      },
      "id": "PFXfwUvSOvGB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GIF to your local computer\n",
        "files.download(filename=gif_file_name)"
      ],
      "metadata": {
        "id": "wwqKE6-5SFpP"
      },
      "id": "wwqKE6-5SFpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tzBEsmuOQQAP"
      },
      "id": "tzBEsmuOQQAP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "text2image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}