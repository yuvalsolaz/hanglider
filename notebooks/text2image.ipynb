{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import image_show_pil, image_show_pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88696845",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda = th.cuda.is_available()\n",
    "device = th.device('cpu' if not has_cuda else 'cuda')\n",
    "print(f'using: {device} device\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sampling parameters\n",
    "batch_size = 1\n",
    "guidance_scale = 3.0\n",
    "diffusion_steps = {'base': '100',       # use diffusion steps for fast sampling   in base model \n",
    "                   'upsample':'fast27'} # use upsample diffusion steps for very fast sampling in upsampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf636cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load / Create base model.\n",
    "from glide_text2im.download import load_checkpoint\n",
    "from glide_text2im.model_creation import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "\n",
    "print(f'use {diffusion_steps} diffusion steps for fast sampling')\n",
    "options = model_and_diffusion_defaults()\n",
    "options['use_fp16'] = has_cuda\n",
    "options['timestep_respacing'] = diffusion_steps['base']  \n",
    "model, diffusion = create_model_and_diffusion(**options)\n",
    "model.eval()\n",
    "if has_cuda:\n",
    "    model.convert_to_fp16()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(load_checkpoint('base', device))\n",
    "print('total base parameters', sum(x.numel() for x in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad6872",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load / Create upsampler model.\n",
    "from glide_text2im.model_creation import model_and_diffusion_defaults_upsampler\n",
    "\n",
    "options_up = model_and_diffusion_defaults_upsampler()\n",
    "options_up['use_fp16'] = has_cuda\n",
    "# options_up['timestep_respacing'] = diffusion_steps['upsample']\n",
    "\n",
    "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
    "model_up.eval()\n",
    "if has_cuda:\n",
    "    model_up.convert_to_fp16()\n",
    "    model_up.to(device)\n",
    "    model_up.load_state_dict(load_checkpoint('upsample', device))\n",
    "\n",
    "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt:str):\n",
    "    # Tune this parameter to control the sharpness of 256x256 images.\n",
    "    # A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
    "    upsample_temp = 0.997\n",
    "\n",
    "    # Create a classifier-free guidance sampling function\n",
    "    def model_fn(x_t, ts, **kwargs):\n",
    "        half = x_t[: len(x_t) // 2]\n",
    "        combined = th.cat([half, half], dim=0)\n",
    "        model_out = model(combined, ts, **kwargs)\n",
    "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
    "        half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
    "        eps = th.cat([half_eps, half_eps], dim=0)\n",
    "        image = th.cat([eps, rest], dim=1)\n",
    "        # image_show_pygame(eps, caption=f'frame #{N}')\n",
    "        # image_show_pygame(rest, caption=f'shape {rest.shape}')\n",
    "        return image\n",
    "\n",
    "    ##############################\n",
    "    # Sample from the base model #\n",
    "    ##############################\n",
    "\n",
    "    # Create the text tokens to feed to the model.\n",
    "    tokens = model.tokenizer.encode(prompt)\n",
    "    tokens, mask = model.tokenizer.padded_tokens_and_mask(tokens, options['text_ctx'])\n",
    "\n",
    "    # Create the classifier-free guidance tokens (empty)\n",
    "    full_batch_size = batch_size * 2\n",
    "    uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask([], options['text_ctx'])\n",
    "\n",
    "    # Pack the tokens together into model kwargs.\n",
    "    model_kwargs = dict(\n",
    "        tokens=th.tensor(\n",
    "            [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
    "        ),\n",
    "        mask=th.tensor(\n",
    "            [mask] * batch_size + [uncond_mask] * batch_size,\n",
    "            dtype=th.bool,\n",
    "            device=device,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Sample from the base model.\n",
    "    model.del_cache()\n",
    "    samples = diffusion.p_sample_loop(\n",
    "        model_fn,\n",
    "        (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
    "        device=device,\n",
    "        clip_denoised=True,\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        cond_fn=None,\n",
    "    )[:batch_size]\n",
    "    model.del_cache()\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958d20d",
   "metadata": {},
   "source": [
    "#### enter image caption for image generation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230eb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20554632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input('enter image caption for image generation: ')\n",
    "print (f'generating image for: {prompt}')\n",
    "samples = generate_image(prompt=prompt)\n",
    "# Show the output\n",
    "image_show_pil(samples, caption=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e523263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
